<!doctype html>
<html lang=en-us>

<head>
    <meta charset=utf-8>
    <meta name=viewport content="width=device-width,initial-scale=1">
    <meta http-equiv=x-ua-compatible content="IE=edge">
    <meta name=generator content="Source Themes Academia 4.3.1">
    <meta name=generator content="Hugo 0.60.1">
    <meta name=author content="Thanushraam Suresh Kumar">
    <meta name=description
        content="Built a full-stack disinfection robot with perception, localization, mapping, and navigation capability.">
    <link rel=alternate hreflang=en-us href="https://thanushraam/featured-projects/Autonomous Racing Car" />
    <meta name=theme-color content="#02b3e4">
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css
        integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css
        integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css
        integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css
        crossorigin=anonymous title=hl-light>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css
        crossorigin=anonymous title=hl-dark disabled>
    <link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
    <link rel=stylesheet href=/css/academia.min.fab247f26624fcebc2e8984cdd070403.css>
    <link rel=manifest href=/site.webmanifest>
    <link rel=stylesheet href="../../styles.css">
    <link rel=icon type=image/png href=/img/icon.png>
    <link rel=apple-touch-icon type=image/png href=/img/icon-192.png>
    <link rel=canonical href=https://thanushraam/featured-projects/Autonomous Racing Car />
    <meta property="twitter:card" content="summary_large_image">
    <meta property="og:site_name" content="Thanushraam Suresh Kumar">
    <meta property="og:url" content="https://thanushraam/featured-projects/Autonomous Racing Car/">
    <meta property="og:title" content="Autonomous Racing Robot | Thanushraam Suresh Kumar">
    <meta property="og:description"
        content="Built a F1 style Racing Car ">
    <meta property="og:image" content="https://thanushraam/featured-projects/Autonomous Racing Car/featured.gif">
    <meta property="twitter:image" content="https://thanushraam/featured-projects/Autonomous Racing Car/featured.gif">
    <meta property="og:locale" content="en-us">
    <meta property="article:published_time" content="2025-12-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2021-03-21T17:50:44-05:00">
    <title>Autonomous Racing Car | Thanushraam Suresh Kumar</title>
</head>

<body id=top data-spy=scroll data-target=#TableOfContents data-offset=71>
    <aside class=search-results id=search>
        <div class=container>
            <section class=search-header>
                <div class="row no-gutters justify-content-between mb-3">
                    <div class=col-6>
                        <h1>Search</h1>
                    </div>
                    <div class="col-6 col-search-close"><a class=js-search href=#><i
                                class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div>
                </div>
                <div id=search-box></div>
            </section>
            <section class=section-search-results>
                <div id=search-hits></div>
            </section>
        </div>
    </aside>
    <nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main>
        <div class=container><a class=navbar-brand href= />Thanushraam Suresh Kumar</a>
            <button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar
                aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button>
            <div class="collapse navbar-collapse" id=navbar>
                <ul class="navbar-nav ml-auto">
                    <li class=nav-item><a class=nav-link href=/#featured-projects><span>Featured Projects</span></a>
                    </li>
                    <li class=nav-item><a class=nav-link href=/#other-projects><span>Other Projects</span></a></li>
                    <li class=nav-item><a class=nav-link href=/#publications><span>Publication</span></a></li>
                    <li class=nav-item><a class=nav-link href=/#about><span>About</span></a></li>
                    <li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li>
                    <li class=nav-item><a class=nav-link href=/files/resume.pdf><span>Resume</span></a></li>
                </ul>
            </div>
        </div>
    </nav>
    <article class="article py-5" itemscope itemtype=http://schema.org/Article>
        <div class="container split-header">
            <div class="row justify-content-center">
                <div class=col-lg-8>
                    <h1 itemprop=name>Autonomous Racing Car</h1>
                    <meta content="2025-12-10 00:00:00 +0000 UTC" itemprop=datePublished>
                    <meta content="2021-03-21 17:50:44 -0500 -0500" itemprop=dateModified>
                    <div class=article-metadata><span class=article-date>Last updated on
                            <time>May 01, 2025</time></span></div>
                    <div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/
                            target=_blank rel=noopener><i class="fab fa-github mr-1"></i>Github Repository</a></div>
                </div>
            </div>
        </div>
        </div>
        <div class=article-container>
            <div class=article-style itemprop=articleBody>
                <p><em>Skills - ROS, C++, Detectron2, LiDAR, PCL, PyTorch, CUDA, Instance Segmentation, Image-LiDAR
                        Fusion, SLAM, Computer Networking</em></p>
                <h2 id=overview>Overview</h2>
                <p>
This project was part of an advanced robotics course where our team—<strong>Team Apollo</strong>—designed and deployed an F1-style autonomous vehicle using a Raspberry Pi-based DeepRacer platform. Originally intended to leverage SLAM and LiDAR-based navigation, the system was re-engineered mid-project due to hardware failure, pivoting fully to computer vision using OpenCV.
</p>

<h2 id="system">System Architecture</h2>
<ul>
  <li><strong>Compute Unit:</strong> Raspberry Pi 4 with remote access enabled via Tailscale VPN, allowing seamless collaboration.</li>
  <li><strong>Vision:</strong> RPi Camera Module (OV5647), horizon-centered for optimal visual coverage.</li>
  <li><strong>Steering:</strong> 15kg servo motor controlling a two-bar linkage for precise front wheel articulation.</li>
  <li><strong>Drive System:</strong> All-wheel-drive (AWD) using a central drive motor and mechanical differentials.</li>
  <li><strong>Telemetry:</strong> Live dashboard using ROS 2 WebSockets for streaming video, speed, steering angle, and CPU temperature.</li>
</ul>

<h2 id="implementation">Implementation Details</h2>
<ul>
  <li><strong>ROS 2 Architecture:</strong> Publisher/subscriber model used to stream color detection data (blue/red/green/yellow) and control signals.</li>
  <li><strong>Line Following:</strong> Based on HSV filtering, the system identified colored lines using contour detection and tracked their centroids. A PID controller handled steering corrections based on positional error.</li>
  <li><strong>Stop Sign Detection:</strong> Detected red-colored signs and halted motion for 4 seconds when the detected contour area exceeded a calibrated threshold.</li>
  <li><strong>Speed Zone Handling:</strong> Reduced speed to 50% in red zones, and restored full speed in green zones, using color detection alone.</li>
  <li><strong>Telemetry Interface:</strong> Displayed speed, steering, camera feed, and CPU temp using ROS topic subscriptions rendered in a web UI via HTML, CSS, and JavaScript.</li>
</ul>

<h2 id="performance">Performance</h2>
<p>
In three timed trial runs, the car recorded lap times of <strong>1:58</strong>, <strong>2:15</strong>, and <strong>2:12</strong>, averaging <strong>2:08 minutes</strong> with a standard deviation of 7.41s. The stop sign and speed limit sign features had ~80% reliability in optimal conditions.
</p>

<h2 id="challenges">Challenges Faced</h2>
<h4>Hardware Issues</h4>
<ul>
  <li><strong>Fan Burnout:</strong> Improper PCB spacing caused the fan to burn out twice due to wiring obstruction. Resolved via a 3D-printed spacer.</li>
  <li><strong>LiDAR Damage:</strong> USB port damage made the LiDAR unusable after attempted PCB repair. Forced pivot to vision-only system.</li>
  <li><strong>Structural Crash:</strong> Robot crashed during tuning tests due to underestimating motor drift; camera & LiDAR mount were reprinted.</li>
  <li><strong>Battery Lockout:</strong> The motor battery locked due to undervoltage and required replacement and reset cables.</li>
</ul>

<h4>Software Issues</h4>
<ul>
  <li><strong>Ubuntu SD Card Corruption:</strong> Required a complete OS reflash mid-project, causing rework and delay.</li>
  <li><strong>Ghost ROS Topics:</strong> Unintended topic overlap caused image data from other robots to interfere with ours—solved by reconfiguring namespaces.</li>
  <li><strong>Motor Drift:</strong> Speed values drifted over time (e.g., 0.0 no longer stopped the motor), leading to constant re-tuning.</li>
</ul>

<h2 id="lessons">Key Lessons</h2>
<ul>
  <li>Hardware assumptions can break easily—design fallback plans early.</li>
  <li>ROS topic management is critical when multiple robots operate in a shared space.</li>
  <li>Team agility matters as much as technical design—rapid pivots helped us deliver all 8 challenge points.</li>
</ul>

                <!-- INSERT VIDEO HERE -->
                <!-- <div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe
                        src=https://www.youtube.com/embed/Hjzabal8oYw
                        style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen
                        title="YouTube Video"></iframe></div> -->
                <p><img src=./figures/demo.gif alt></p>
                <h2 id=hardware>Hardware</h2>
                <ul>
                    <li>Raspberry Pi</li>
                    <li>RP-LiDAR</li>
                    <!-- <li></li> -->
                </ul>
                <p>📄 <a href="./Robotics_Final_Report.pdf" target="_blank">View Full Project Report (PDF)</a></p>
                <!-- <p>The RGB-D camera is mounted at shoulder height by an Aluminum extrusion. The final version of the
                    perception module's inputs are RGB images from the Realsense camera and 3D point cloud from Velodyne
                    LiDAR.</p> -->
                <!-- <p><img src=figures/hardware.gif alt></p>
                <h2 id=software>Software</h2>
                <p>The diagram below shows the high-level overview of the software stack with inputs and outputs for
                    each sub-system.</p>
                <p><img src=figures/software.jpg alt></p>
                <p>The operation pipeline includes three stages.</p>
                <p><strong>Stage 1 - SLAM:</strong> In this stage, manually driving the robot around builds a map of its
                    surrounding environment. The map includes a 2D occupancy grid and a pose graph for localization in
                    later stages.</p>
                <p><strong>Stage 2 - Object-level Map Building:</strong> In the second stage, the robot localizes itself
                    in the environment in real-time. It moves around to detect and classify all the tables and chairs in
                    the environment using an image-LiDAR fusion-based approach. Objects are detected and segmented by
                    Mask R-CNN. Detectron2 object detection and segmentation framework was used, and the model was
                    pre-trained on the COCO dataset. With an extrinsic calibration between the camera and the 3D LiDAR,
                    the 3D point cloud can be projected onto the image plane and overlap with the image. The point cloud
                    inside each mask can then be extracted. Because there will be foreground and background points
                    getting included, clustering algorithms were used for post-processing.</p>
                <p>Each object is represented as a 2D convex hull. Because each time, the robot can only look at an
                    object from one direction, which gives an incomplete representation of the object, a Hungarian
                    algorithm is also implemented for data association (<a
                        href=https://github.com/shangzhouye/disinfection-robot-ros/blob/master/rgbd_object_detection/src/object_map_server.py><code>object_map_server</code></a>)
                    running in the backend. When two convex hulls are associated, which means they represent the same
                    object, they will get merged into a large convex hull so that by looking at the same object from
                    different directions, a complete representation for each object can be obtained. After this stage,
                    an object-level map representing all the tables and chairs in the environment is built.</p>
                <p><strong>Stage 3 - People Detection:</strong> This module detects and tracks people with a Kalman
                    filter. The <a
                        href=https://github.com/shangzhouye/disinfection-robot-ros/blob/master/rgbd_object_detection/src/object_map_server.py><code>object_map_server</code></a>
                    then detects which tables/chairs have been used and need disinfection. When a table is being used,
                    it turns green in the visualization. When it passes a certain threshold, the convex hull permanently
                    turns yellow with a &lsquo;need disinfection&rsquo; tag.</p>
                <p>The images below show a detailed pipeline of the image-LiDAR fusion algorithm.</p>
                <p><img src=./figures/fusion_block.jpg alt></p>
                <p><img src=./figures/fusion_detail.jpg alt></p>
                <h2 id=inference-in-the-cloud>Inference in the cloud</h2>
                <p>The software stack has the computation done both at the edge and &lsquo;in the cloud.&rsquo; Because
                    there is no GPU on the robot, a local network is set up between three Linux machines: the robot, the
                    GPU computer in the lab, and my laptop for visualization. Nodes are split onto the onboard computer
                    (Jackal robot) and the cloud GPU computer (the Beast computer). Image data is getting transferred
                    onto the GPU computer wirelessly under the ROS framework for deep learning inference, and then and
                    results are sent to my laptop for visualization.</p>
                <p>The diagram below shows which node is running on which computer.</p>
                <p><img src=./figures/cloud.jpg alt></p>
                <p>For hostname resolution details and the ROS setup to perform this hybrid computing, please refer to
                    this <a
                        href=https://github.com/shangzhouye/disinfection-robot-ros/blob/master/README.md/#setup-documentation>README</a>.
                    Inference in the cloud (with a GeForce GTX 1080Ti) is able to achieve a 6Hz frame rate.</p> -->
            </div>
            <div class="media author-card" itemscope itemtype=http://schema.org/Person><img class="portrait mr-3"
                    src="../../media/1000010322.JPEG"
                    itemprop=image alt=Avatar>
                <div class=media-body>
                    <h5 class=card-title itemprop=name><a href=https://thanushraam>Thanushraam Suresh Kumar</a></h5>
                    <h6 class=card-subtitle>M.S. Robotics Student</h6>
                    <ul class=network-icon aria-hidden=true>
                        <li><a itemprop=sameAs href=mailto:thanushraam.sureshkumar@colorado.edu><i
                                    class="fas fa-envelope"></i></a></li>
                        <li><a itemprop=sameAs href=https://www.linkedin.com/in/thanushraam-suresh-kumar/ target=_blank
                                rel=noopener><i class="fab fa-linkedin"></i></a></li>
                        <!-- <li><a itemprop=sameAs href=https://www.youtube.com/
                                target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li> -->
                        <li><a itemprop=sameAs href=https://github.com/tr0612 target=_blank rel=noopener><i
                                    class="fab fa-github"></i></a></li>
                    </ul>
                </div>
            </div>
        </div>
    </article>
    <script src=/js/mathjax-config.js></script>
    <script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
    <script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js
        integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
    <script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js
        integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
    <script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js
        integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
    <script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js
        integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full"
        integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin=anonymous async></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src=/js/academia.min.281fa180ccfa1769ea3c7dd430814861.js></script>
    <div class=container>
        <footer class=site-footer>
            <div class=container>
                <div class=row>
                    <div class=col-md-6>
                        <p>&copy; Thanushraam Suresh Kumar, 2025</p>
                    </div>
                    <div class=col-md-6>
                        <ul class="list-inline network-icon text-right"></ul>
                    </div>
                </div>
            </div>
        </footer>
    </div>
    <div id=modal class="modal fade" role=dialog>
        <div class=modal-dialog>
            <div class=modal-content>
                <div class=modal-header>
                    <h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
                        <span aria-hidden=true>&#215;</span></button>
                </div>
                <div class=modal-body>
                    <pre><code class="tex hljs"></code></pre>
                </div>
                <div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i
                            class="fas fa-copy"></i>Copy</a>
                    <a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i
                            class="fas fa-download"></i>Download</a>
                    <div id=modal-error></div>
                </div>
            </div>
        </div>
    </div>
</body>

</html>